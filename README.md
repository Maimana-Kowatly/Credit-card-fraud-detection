# ğŸ›¡ï¸ Credit Card Fraud Detection using Machine Learning

In todayâ€™s digital economy, credit card fraud is a pressing challenge that threatens both consumers and financial institutions. This project tackles fraud detection by developing robust machine learning models that emphasize **recall**, ensuring fraudulent transactions are caught even in the presence of extreme class imbalance.

---

## ğŸ“Œ Objective

To build and evaluate machine learning models capable of accurately identifying fraudulent credit card transactions within a highly imbalanced dataset, with a strong focus on **maximizing recall**.

---

## ğŸ’¼ Business Context

- Global credit card fraud losses are estimated in the **billions of dollars**.
- False **negatives** (missed frauds) can lead to significant financial and reputational damage.
- False **positives** (flagging valid transactions) can frustrate customers and disrupt trust.
- Hence, this project prioritizes **sensitivity (recall)** over simple accuracy.

---

## ğŸ“Š Dataset Overview

- **Source**: Kaggle - Credit Card Fraud Detection dataset
- **Transactions**: ~284,807 total
- **Fraudulent**: < 0.2% (~492 cases)
- **Features**:
  - `V1` to `V28`: PCA-transformed, anonymized features
  - `Time`: Seconds since first transaction
  - `Amount`: Transaction amount
  - `Class`: Target variable (1 = fraud, 0 = legitimate)

---

## ğŸ§ª Methodology

### 1. ğŸ“ˆ Exploratory Data Analysis (EDA)
- Uncovered extreme class imbalance
- Visualized distributions and fraud patterns
- Guided choice of preprocessing and sampling strategies

### 2. âš™ï¸ Preprocessing
- **Stratified split** to maintain fraud ratio
- **Feature scaling** to normalize input distributions

### 3. âš–ï¸ Handling Class Imbalance
- Compared four strategies:
  - **Imbalanced**: Original skewed dataset
  - **Random Oversampling**
  - **SMOTE**
  - **ADASYN** (adaptive synthetic sampling)

### 4. ğŸ¤– Model Development
- **Logistic Regression**: Baseline
- **Decision Tree**: Captures non-linearities
- **XGBoost**: High-performance, ensemble-based model

Each algorithm was trained under all four sampling methods.

---

## ğŸ† Best Model

- **Model**: XGBoost
- **Sampling Strategy**: ADASYN
- **Metrics**:
  - **ROC AUC**: ~0.98
  - **Recall**: **0.86**
  - Balanced performance, especially on detecting fraudulent cases

ğŸ“Œ The combination of **XGBoost + ADASYN** achieved the best balance between recall and precision, proving effective in catching hard-to-detect fraud cases.

---

## ğŸ“ Files

- `.ipynb`: Main Jupyter notebook containing code, analysis, and visualizations
- `README.md`: Project overview and documentation

---

## ğŸ§  Key Learnings

- Imbalanced data requires **specialized preprocessing** and **targeted sampling**.
- **Recall** is crucial in fraud detection, more than overall accuracy.
- XGBoost, combined with **adaptive sampling**, can effectively handle class imbalance and improve fraud detection rates.

---

## ğŸ“š How to Run

1. Clone the repository
2. run the .ipynb files
